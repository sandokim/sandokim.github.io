---
title: "[생성모델 연구] GAN 발전동향 정리"
last_modified_at: 2024-05-31
categories:
  - 연구
tags:
  - 연구
  - GAN
  - mode collapse
excerpt: "GAN 발전동향 정리"
use_math: true
classes: wide
---

### GAN 학습과 관련된 설명

1. **p(data)**: 
   - 여기서 x1, x2, x3, x4는 특정 샘플 이미지의 표현 벡터를 나타내는 것이 아니라, 고차원 공간에서 해당 샘플 이미지가 분포할 확률을 의미합니다.

2. **Discriminator 학습**:
   - GAN을 학습할 때, Discriminator를 학습할 때는 Generator를 고정한 채로 Discriminator를 학습시킵니다. 이때 Generator는 이미지만 생성하는 역할을 합니다.

3. **Generator 학습**:
   - GAN을 학습할 때, Generator를 학습할 때는 Discriminator를 고정한 채로 Generator를 학습시킵니다. 이때 Discriminator는 Real과 Fake만 구분하는 역할을 합니다.

4. **분리된 학습 방식**:
   - Generator와 Discriminator는 각각 따로 학습하는 방식을 취합니다.

5. **Mode collapse 문제**:
   - Generator와 Discriminator가 경쟁적으로 (따로따로) 학습할 때, Discriminator가 너무 빨리 학습되면 Generator가 제대로 된 이미지를 생성하지 못하는 경우가 발생하는데, 이를 Mode collapse라고 명명합니다. 이를 해결하기 위해 Generator를 더 많이 학습시키는 방식이 있었으나, 현재 트렌드는 loss 측면에서 이 문제를 해결하는 방향으로 발전하고 있습니다.

6. **Generator weight 공유**:
   - 일반적으로 Generator weight만 GitHub에서 공유하며, Discriminator weight는 공개하지 않습니다. 이는 Generator가 이미지를 잘 생성하는 것이 가장 중요하기 때문입니다.

7. **Latent vector 조작**:
   - "man with glasses - man without glasses + woman without glasses = woman with glasses"라는 변환을 할 때 각 특징에 해당하는 latent vector를 평균하여 더하고 빼서 이미지를 생성합니다. 이미지에 변화를 주기 위해서는 전체적으로 작은 노이즈를 latent vector에 더해주는 방식을 취합니다.

8. **Conditional GAN (CGAN)**:
   - z를 input으로 줄 때, z (1,256)와 y (1,10)을 concat하여 (1,266)으로 만들어서 입력합니다. Generator에서 latent vector z와 conditional vector y를 concat하여 입력하고, Real image를 넣을 때도 Real에 y를 concat하여 Discriminator에서도 y를 조건으로 입력하는 방식입니다. 이 부분이 그림에서 명확히 표현되지 않아 혼선이 있을 수 있습니다.

9. **Semi Supervised GAN (SGAN)**:
   - SGAN은 스스로 condition을 찾도록 학습하는 네트워크입니다.

### GAN 학습의 발전

1. **초기 단계 학습**:
   - 학습 초기 단계에서 Generator가 생성하는 데이터는 좋지 않기 때문에 Discriminator의 accuracy가 100%입니다. 그러나 학습이 진행되면서 Generator의 성능이 향상되어 Real과 가까운 이미지를 생성하게 되면 Discriminator의 accuracy는 점차 떨어져 이상적으로는 50%에 도달합니다. Ideal한 Discriminator의 학습 그래프를 그려보면 Discriminator의 accuracy가 60%에 도달하는 것을 보여줍니다.

2. **Discriminator의 실제 accuracy**:
   - 실제 Discriminator의 accuracy는 학습 초기에는 보통 구분력이 없기 때문에 100%에서 시작하는 것이 아니라, 0%에서 100%로 올라가는 형태로 관찰됩니다. 또는 0%에서 시작해 spike가 생겨 100%를 찍었다가 떨어져서 60~70%에 수렴하는 형태를 보이기도 합니다.

3. **Discriminator accuracy 기반 학습 수렴 결정**:
   - Discriminator의 accuracy를 기반으로 학습의 수렴을 결정하면 40%~60%에서 변동폭이 심할 수 있습니다. 그러나 Discriminator의 accuracy가 60%라고 해서 Generator가 생성한 이미지가 꼭 좋은 것은 아니고, Discriminator의 accuracy가 40%라고 해서 Generator가 생성한 이미지가 꼭 나쁜 것은 아닙니다. 따라서 Discriminator의 accuracy만으로 성능의 최적점을 평가하기에는 부적절합니다.

4. **Logit 값을 이용한 학습**:
   - 이 문제를 해결하기 위해 Discriminator의 accuracy만으로 학습하는 것이 아니라, Discriminator가 Real인지 Fake인지 구분한 샘플들의 logit 값들의 기대값을 구하여 이를 Real, Fake의 augmentation 기준으로 사용합니다. 만약 r=1이면 Discriminator가 Real을 Real이라고만 구분하는 상황이므로 overfitting이 되었다고 볼 수 있습니다. r=0이면 Discriminator가 Real과 Fake를 헷갈리지 않는 상황이며, 우리가 원하는 방향입니다. Augmentation의 강도를 조절하여 Discriminator의 학습 난이도를 조정할 수 있습니다.

5. **Sigmoid와 logit 값**:
   - Sigmoid를 사용하면 0~1 사이로 변환되는데, Sigmoid에 넣기 전의 Discriminator의 logit 값을 r을 구할 때 사용합니다.

6. **균형 잡힌 학습**:
   - Discriminator와 Generator가 균형 있게 학습하는 것이 중요합니다. Discriminator가 Generator보다 살짝 성능이 좋아야 하지만, 너무 성능이 좋으면 Real을 Real로, Fake를 Fake로만 구분하게 되어 Mode collapse에 빠질 수 있습니다.

7. **학습 주기 조절**:
   - 학습 밸런스가 잘 맞지 않아 Generator와 Discriminator의 학습 주기를 iteration마다 Generator는 1번, Discriminator는 5번으로 설정하는 기법을 사용하기도 했습니다. 그러나 이는 근본적인 해결책이 아니므로, 이후 발전된 Discriminator에서는 patch 단위로 least square로 구하여 loss를 더 복잡하게 만들어 균형 있게 학습하도록 발전하였고, Discriminator와 Generator의 학습 주기는 1:1로 학습하는 방향으로 발전하였습니다.

