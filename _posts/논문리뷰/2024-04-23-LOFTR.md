---
title: "[논문리뷰] LoFTR: Detector-Free Local Feature Matching with Transformers"
last_modified_at: 2024-04-23
categories:
  - 논문리뷰
tags:
  - Computer Vision
  - AI
  - SIGGRAPH
  - Local Features
excerpt: "LOFTR 논문 리뷰"
use_math: true
classes: wide
---

> CVPR 2021. [[Paper](https://arxiv.org/pdf/2104.00680.pdf)] [[Page](https://zju3dv.github.io/loftr/)] [[Github](https://github.com/zju3dv/LoFTR)] [[5-minute Summary](https://www.youtube.com/watch?v=ep015Dda0T0)]
> Sun, Jiaming and Shen, Zehong and Wang, Yuang and Bao, Hujun and Zhou, Xiaowei
> 1 Apr 2021

![image](https://github.com/sandokim/sandokim.github.io/assets/74639652/845a74d8-7d43-4adb-b329-896c3b1f56ab)
LOFTR는 트랜스포머 이미지와 함께 Local Feature Matching을 제시합니다. 
Local Feature Matching은 모션 비주얼, 위치화, 및 슬램과 같은 많은 세 가지 분할 작업에 중요합니다. 
Local Feature Matching을 위한 표준 파이프라인은 특징 감지, 설명, 그리고 매칭으로 나눌 수 있습니다. 
그러나 이러한 파이프라인은 몇 가지 문제점을 가지고 있습니다. 

첫째로, Feature Detector는 텍스처가 낮거나 반복 패턴이 많은 경우에 실패할 수 있습니다. 
![image](https://github.com/sandokim/sandokim.github.io/assets/74639652/fd0adda7-1685-4699-a612-7fb01d39c53b)

둘째로, Local Descriptor는 위치에 무관하므로 잘못된 매칭이 발생할 수 있습니다.
![image](https://github.com/sandokim/sandokim.github.io/assets/74639652/998ab366-b351-4657-97b5-f9d69547ded6)


LOFTR는 이러한 문제를 해결하기 위해 탐지기 없는 파이프라인을 사용합니다. 
![image](https://github.com/sandokim/sandokim.github.io/assets/74639652/a3b27636-93f4-4887-9a14-1dc7257973a6)
이를 통해 특징의 반복성 병목 현상을 피하고 정확한 매칭을 가능하게 합니다. 
LOFTR는 교차 수준에서 일치 항목을 추출하고, 그 후에 하위 픽셀에 대한 일치 항목을 세분화하여 정확한 매칭을 수행합니다. 
이를 통해 반복성 병목 현상을 방지하고 정확한 매칭을 가능하게 합니다.

LOFTR는 Feature Detector의 반복성 병목 현상을 피하기 위해 탐지기 없는 파이프라인을 사용하고 위치 종속 특징 표현을 추출하기 위한 transformer를 사용합니다. 
이는 특징 검출기가 없는 LOFTR의 개요입니다. 
LOFTR는 먼저 Coarse Level에서 dense matches를 추출하고, 나중에 refines the good matches to subpixel합니다. 
이는 이미지 쌍의 각 영역이 잠재적으로 일치할 수 있음을 의미합니다. 
반복성 병목 현상이 방지되며, fine level matching은 정확한 pose estimation을 보장하기 위해 coarse matches 위치를 더욱 구체화합니다.

LOFTR에는 4개의 모듈이 있습니다. 
![image](https://github.com/sandokim/sandokim.github.io/assets/74639652/403b0eab-bf73-4c62-b4f4-70e7356d2a1d)
중간 결과의 일부 시각화를 통해 각 모듈을 살펴볼 수 있습니다. 
로컬 기능 cnn은 간단한 인코더 디코더 스타일 장면으로, 두 가지 수준의 특징 맵을 추출하는 CNN은 특징 맵의 차원을 줄여 나중 모듈에서 계산 비용을 관리 가능한 수준으로 유지합니다. 
로컬 특징 장면의 코스 특징 맵은 1차원으로 평면화되고 위치 인코딩과 결합됩니다. 
transformer는 interleaved된 self and cross attention layer를 포함하는 모듈로, coarse level local features을 여러 번 처리하고 변환하는 데 사용됩니다.

![image](https://github.com/sandokim/sandokim.github.io/assets/74639652/f29ed8ed-38e5-47ad-aa84-095cf71991c0)
우측의 transformed features의 color gradient를 보면, LOFTR module이 feature space에서 originally close한 local features를 서로 distant하게 transform하는 것을 볼 수 있습니다. A와 B를 보면 texture-less wall이므로 local features만 봤다면 둘 간의 차이가 없어서 color gradient가 같을 것입니다. 즉, transformed features는 input image의 local appearances에만 dependent한게 아니라 두 이미지간의 positions에도 dependent하다는 것을 보여줍니다!


LOFTR는 세 가지 데이터 세트에 대해 평가되었습니다.
LOFTR는 성능이 뛰어나거나 대부분의 최첨단 방법과 동등하며, 공개 벤치마크에서 1위를 차지했습니다.
![image](https://github.com/sandokim/sandokim.github.io/assets/74639652/5986c412-93d2-45e4-814b-64fc232cb11d)
state-of-the-art detector-based feature matching method인 SuperGlue와 비교해봅시다. textureless regions에서 interest points가 detected되지 않기 때문에 SuperGlue는 오직 good textures가 있는 regions에 대하서만 matches를 생성합니다. 반면에 LOFTR는 entire co-visible area에 well distributed된 semi-dense matches를 생성합니다. 

![image](https://github.com/sandokim/sandokim.github.io/assets/74639652/91f960f5-62f9-453e-9552-5793ab119f78)

