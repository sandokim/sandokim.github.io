---
title: "[4D] TiNeuVox: Fast Dynamic Radiance Fields with Time-Aware Neural Voxels"
last_modified_at: 2025-01-15
categories:
  - 4D
tags:
  - 4D
  - TiNeuVox
  - Time-Aware Neural Voxels
  - dynamic scenes
excerpt: "TiNeuVox"
use_math: true
classes: wide
comments: true
---

[Fast Dynamic Radiance Fields with Time-Aware Neural Voxels](https://arxiv.org/abs/2205.15285)

Explict data structures, e.g. voxel features, show great potential to accelerate the training process.

However, voxel features face two big challenges to be applied to dynamic scenes, i.e. 

- **modeling temporal information**
- **capturing different scales of point motions**

...

**Especially, most existing methods model dynamic scenes by introducing an additional deformation network with a similar scale of the radiance network, which maps point coordinates into a canonical space.**

**This manner means much more cost for training and inferring dynamic fields.**

The cumbersome time cost impedes wide applications in real-life scenarios.

One direct and simple solution is to expand the voxel grids with an additional time dimension.

However, this manner will undoubtedly increase memory cost significantly.

Changing voxel grids from 4D to 5D, i.e. $(C, N_x, N_y, N_z) \rightarrow (C, N_x, N_y, N_z, N_t)$, will multiply the storage cost by $N_t$.

**On the other hand, there usually exist motions of different scales.**

**Voxels with the high resolutions locate in small grids, which fail to model large motions; voxels in large grids fail to capture details with small motions.**

### 핵심요약
- **dynamic fields 학습 소요시간을 줄이기 위해 explicit data structures를 사용**
- **large motions, small motions을 modeling하기 위해 multi-distance interpolation을 사용**

**To encode temporal information, we first build a highly compressed deformation network which maps 3D point coordinates into a coarse canonical space.**

**Voxel features are queried with the transformed coordinates.**

We further enhance the temporal information by feeding time and coordinate embeddings into the latter radiance network.

Thus deviation introduced by point mapping can be automatically suppressed by the neural network.

**Moreover, we propose a multi-distance interpolation method, where features are obtained from voxels with multiple distances.**

**In this way, both small and large motions can be modeled even though only one single-resolution voxel features are constructed.**

### 2 Related Works
#### 2.1 Neural Rendering for Dynamic Scenes
The key problem to solve dynamic-scene rendering lies in temporal information encoding. One stream of dynamic NeRF methods model deformations in scenes by extending radiance fields with an additional time dimension.
#### 2.2 Neural Rendering Acceleration
Some works propose to represent scenes with explicit voxel-grid features/properties and directly optimize these voxels for extremely fast convergence speed, reducing training time from hours to minutes.

However, storage cost is significantly increased for storing voxel features.

Recent works effectively reduce the storage cost via voxel hashing, tensor decomposition, and bitrate dicionary lookup while still maintaining surprisingly high training speed.

These voxel-optimizable methods yet only focus on static scenes, where voxel features are direct to construct for only spatial information.

**We introduce optimizable explicit voxel features into dynamic scenes.**

**Temporal information is encoded to obtain time-aware neural voxel features.**

### 3 Method
#### 3.2 Multi-Distance Interpolation with Nerual Voxels
To predict the property of one 3D point, neural voxels stored in eight vertices of the grid this point lies in are queried and interpolated trilinearly.

**The interpolated feature is then inferred by _neural networks_ to predict the expected properties.**
  - These interpolated voxel features are first positional-encoded as in Eq. 4 and being fed into the neural network for inference.

![image](https://github.com/user-attachments/assets/a15f6dce-8a6b-4016-8cef-522ef80567e7)

Considering points in dynamic scenes may move with a dramatic motion trajectory, small grids of neural voxels have limited capacity to model these point movements.

We propose a multi-distance interpolation method to model point motions with various scales.

As shown in Fig. 3, besides the smallest grid, voxel features are also interpolated from vertices of larger grids.

This means the final features for inference not only come from nearest voxels, but also from sub- and subsub-nearest voxels.

In this way, small motions can be modeled via near voxels while motions in a large region are perceived with farther voxels.






